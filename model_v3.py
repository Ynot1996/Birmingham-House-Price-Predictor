import matplotlib

# Force the backend to TkAgg for macOS window compatibility
matplotlib.use('TkAgg')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# 1. Load the V2 Feature Files (generated by your feature_engineering_v2.py)
print("Loading V2 feature datasets...")
train_df = pd.read_csv('train_features_v2.csv')
test_df = pd.read_csv('test_features_v2.csv')

# 2. Alignment: Ensure both datasets have the exact same columns
common_cols = list(set(train_df.columns) & set(test_df.columns))
train_df = train_df[common_cols]
test_df = test_df[common_cols]

# 3. Define Features (X) and Target (y)
# We use log transformation on Price to normalize the distribution
X_train = train_df.drop('Price', axis=1)
y_train_log = np.log1p(train_df['Price'])

X_test = test_df.drop('Price', axis=1)
y_test_real = test_df['Price']  # Keep real prices for final evaluation metrics

# 4. Train the Random Forest Model
# This model will now leverage 'Area_Avg_Price' as a primary predictor
print("Training Model V3 (Random Forest with Target Encoding)...")
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train_log)

# 5. Make Predictions and Revert Log Transformation
predictions_log = model.predict(X_test)
predictions_real = np.expm1(predictions_log)  # Convert log back to GBP

# 6. Evaluation Metrics
mae = mean_absolute_error(y_test_real, predictions_real)
r2 = r2_score(y_test_real, predictions_real)

print("-" * 30)
print(f"Model V3 Results:")
print(f"Average Error (MAE): £{mae:.2f}")
print(f"Model Reliability (R2): {r2:.4f}")


# 7. VISUALIZATION: Updated Feature Importance
def plot_feature_importance_v3(model, X_train):
    importances = model.feature_importances_
    feature_names = X_train.columns

    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance',
        ascending=False)

    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=importance_df.head(15), palette='viridis', hue='Feature',
        legend=False)
    plt.title('V3 Feature Importance: The Impact of Target Encoding', fontsize=16)
    plt.xlabel('Importance Score', fontsize=12)
    plt.tight_layout()
    plt.savefig('feature_importance_v3.png')
    print("Success: Saved feature_importance_v3.png")
    plt.show(block=True)


# 8. VISUALIZATION: Actual vs Predicted
def plot_results_v3(y_test, predictions):
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, predictions, alpha=0.3, color='teal')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual Price (£)')
    plt.ylabel('Predicted Price (£)')
    plt.title('Model V3: Actual vs Predicted (Mainstream Market)')
    plt.tight_layout()
    plt.savefig('results_v3.png')
    print("Success: Saved results_v3.png")
    plt.show(block=True)


# Run Visualizations
plot_feature_importance_v3(model, X_train)
plot_results_v3(y_test_real, predictions_real)